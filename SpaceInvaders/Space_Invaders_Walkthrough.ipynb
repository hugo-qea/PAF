{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook réalisé par Aristide LALOUX, Hugo QUENIAT et Mohamed Ali SRIR. \n",
    "# Sujet : \n",
    "Ce notebook contient le code nécessaire pour créer et entrainer un agent à jouer à Space Invaders sur Atari 2600.\n",
    "Nous utilisons ici un réseau de neurones convolutionnel (CNN=Convolutional Neuronal Network). C'est à dire que l'agent fait passer l'image en cours du jeu dans une boîte noire qui lui renvoie l'action optimale à jouer. \n",
    "Nous y utiliserons donc un algorithme de type DQN (Deep Q Network) qui correspond à du Deep Q Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie :\n",
    "Nicolas Renotte's code :\n",
    "- Tutorial : https://www.youtube.com/watch?v=hCeJeq8U0lo\n",
    "- Git Repository : https://github.com/nicknochnack/KerasRL-OpenAI-Atari-SpaceInvadersv0/actions \n",
    "- Agent entrainé avec 1M de steps : https://drive.google.com/file/d/1TgfGittIQC2KhNbut2l4NSASUr0swe1u/edit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les installations des bibliotèques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par installer les différentes librairies que l'on va utiliser.\n",
    "Il nous faut :\n",
    "- L'environnement de jeu Space Invaders sur Atari 2600.\n",
    "- Un agent et de quoi créer un CNN.\n",
    "- L'algorithme de DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Installation des différentes librairies que l'on va utiliser\n",
    "\n",
    "#1 Gym pour l'environnement du jeu : C'est à dire l'ensemble S des états, l'ensemble A des actions et l'ensemble R des récompenses.\n",
    "# L'environnement se trouve dans un ROM qui permet d'émuler le jeu sur une machine autre que la console atari.\n",
    "!{sys.executable} -m pip install autorom[accept-rom-license]\n",
    "!{sys.executable} -m pip install gym[accept-rom-license]\n",
    "!{sys.executable} -m pip install gym gym[atari]\n",
    "\n",
    "#2 Il faut ensuite installer les librairies de Réseaux Neuronaux et d'algorithmes de Deep Q Network (DQN) \n",
    "!{sys.executable} -m pip install tensorflow keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de l'environnement de jeu et premier essai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imports des bibliothèques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importons les bibliothèques et packages qui permettent de créer l'environnement de SpaceInvaders\n",
    "import gym #La bibliothèque gym contient beaucoup d'environnements déjà codés et notamment SpaceInvaders\n",
    "import random #La bibliothèque random nous permettra de choisir des actions aléatoires ou bien d'implémenter l'epsilon-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Création de l'environnement de jeu Space Invaders Atari 2600:\n",
    "\n",
    "\n",
    "Les états correspondent à toutes les possibilités de position de notre vaisseau et des vaisseaux ennemis sur l'écran. Cela fait beaucoup d'états possibles, c'est pourquoi on utilise le DQN plutôt qu'un QL basique.\n",
    "Il y a 6 actions : \n",
    "\n",
    "{Ne_rien_faire / Tirer / Déplacement_Droite / Délacement_Gauche / Déplacement_Droite & Tirer / Déplacement_Gauche & Tirer}.\n",
    "\n",
    "Les récompenses correspondent simplement au score dans le jeu. Celui-ci augmente à chaque vaisseau ennemi détruit, la récompense obtenue à chaque vaisseau éliminé étant proportinnelle à la distance entre le joueur et le vaisseau.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\envs\\registration.py:564: UserWarning: \u001b[33mWARN: The environment SpaceInvaders-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#On importe l'environnement Gym SpaceInvaders que l'on retrouve directement sur leur site internet. \n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "#On récupère la taille des images que l'environnement génère. \n",
    "height, width, channels = env.observation_space.shape\n",
    "print(env.observation_space.shape) \n",
    "\n",
    "#On récupère les actions permises à la futur IA. (L'ensemble A)\n",
    "env.unwrapped.get_action_meanings()\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "actions = env.action_space.n\n",
    "print(env.action_space.n)\n",
    "#Ici on voit qu'il y a 6 actions contenues dans une liste. L'action 1 correspond à FIRE (donc tiré), 4 à LEFT (donc déplacer notre\n",
    "#vaisseau d'une unité à gauche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons le temps de faire jouer un agent de manière totalement aléatoire. Premièrement, cela permet de se familiariser à la syntaxe proposée par Gym. Ensuite, il sera intéressant de comparer les résultats d'une tentative de jeu totalement aléatoire avec une tentative de jeu d'une IA entrainée. On espère évidemment que notre agent entrainé fasse mieux qu'une tactique de jeu aléatoire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gym\\utils\\passive_env_checker.py:305: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:65.0\n",
      "Episode:2 Score:205.0\n",
      "Episode:3 Score:140.0\n",
      "Episode:4 Score:120.0\n",
      "Episode:5 Score:110.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 #On va lancer 5 épisodes = tentatives de jeu.\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset() #On reset la partie à chaque début de partie, logique!\n",
    "    done = False #Done vaut false tant que l'agent n'est pas mort ou n'a pas gagné le jeu. Done indique ainsi une fin de partie.\n",
    "    score = 0 #Le score vaut la somme des rewards obtenus au fur et à mesure des itérations. \n",
    "    \n",
    "    while not done: #Tant que le jeu n'est pas fini : IE l'agent n'est pas mort ou n'a pas éliminé tous les ennemis.\n",
    "        env.render(mode=\"rgb_array\") #Pour afficher la fenêtre de jeu. \n",
    "        action = random.choice([0,1,2,3,4,5]) #On prend ici des actions totalement aléatoires.\n",
    "        n_state, reward, done, info = env.step(action) #La fonction step de gym s'occupe d'appliquer l'action \"action\" à l'état actuel pour \n",
    "        #calculer le prochain état. \n",
    "        score+=reward #On stocke le cumul des rewards dans \"score\"\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création du CNN (Convolutionnal Neuronal Network) avec Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous allons ici implémenter notre réseau neuronal. Il faut que l'IA soit capable de prendre une image en entrée et de ressortir la meilleur action à jouer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\Users\\lalou\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential #Bibliothèque pour créer un CNN de manière \"séquential\", c'est à dire qu'on va lui ajouter de manière séquentielle des couches une à une. \n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D #Bibliothèques des couches de CNN.\n",
    "from tensorflow.keras.optimizers import Adam #Algo pour optimiser l'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On écrit la fonction qui permet d'implémenter le réseau neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions): #On lui passe en argument la taille des images en entrée et la taille de la liste des actions.\n",
    "    \n",
    "    model = Sequential() #On déclare que noous allons construire le modèle de façon séquentielle, ie on va lui ajouter successivement des couches. \n",
    "\n",
    "    #On commence par des couches de convolution pour traiter les images.\n",
    "    \n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    #Ici : 32 correspond au nombre de filtres, (8,8) correspond à la taille de la matrice du filtre, strides=(4,4) correspond au pas de placement\n",
    "    #du filtre entre deux calculs et activation='relu' correspond à la fonction d'activatin de couche qui est ici une relu.\n",
    "\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    #Idem\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    #Idem. Attention ici strides n'est pas précisé, par défault il vaut 1 et donc le filtre se déplace de pixel en pixel et passe donc\n",
    "    #par tous les pixels.\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #Cette couche permet de reprendre toutes les imagettes calculées en sortie des 32*64*64 filtres et de tout mettre dans un grand tableau. \n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #Il s'agit ici d'une couche fully-connected de taille 512 en sortie avec une fonction d'activation de type \"relu\"\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    #Il s'agit ici d'une couche fully-connected de taille 256 en sortie avec une fonction d'activation de type \"relu\"\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    #Il s'agit ici d'une couche fully-connected de taille actions/6 en sortie avec une fonction d'activation de type \"linéaire\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous créons à présent le réseau neuronal. Il sera stocké dans la variable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous pouvons obtenir des informations sur les couches de notre réseau : input, output, dimensions, nombre de variable, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "#Pour le calcul des dimensions à chaque couche https://www.baeldung.com/cs/convolutional-layer-size\n",
    "\n",
    "#Pour une couche convolutive, avec un filtre, on a les formules suivantes:\n",
    "#Height_output = (Height_input - Height_filter)/(HeightStridesFilter) +1\n",
    "#width_output = (Witdh_input - Witdh_filter)/(WitdhStridesFilter) +1\n",
    "\n",
    "#On retrouve bien les valeurs que le tableau contient. \n",
    "#Remarque : le \"3\" étant pour le RGB des pixels (Red, Green, Blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de l'agent à l'aide de Keras.RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous entrons à présent dans la partie qui nous intéresse particulièrement : nous allons entrainer notre agent pour jouer à Space Invaders.\n",
    "- Commençons par importer les librairies python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous allons utiliser la bibliothèque DQNAgent de Keras RL qui facilite la création de notre Agent pour l'algorithme de DQN\n",
    "from rl.agents import DQNAgent\n",
    "\n",
    "\n",
    "#Notre agent à besoin d'un buffer/mémoire pour y stocker les informations qu'il a appris lors de son entrainement.\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "#Enfin les politiques epsilon greedy et epsilon greedy décroissante existent déjà dans les bibliothèques ci dessous.\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "#EpsGreedyQPolicy correspond à une politique EpsilonGreedy où Epsilon est constant\n",
    "#LinearAnnealedPolicy correspond à une politique EpsilonGreedy où Epsilon décroit linéairement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Créons donc la fonction d'initialisation de notre Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    #Nous devons définir la politique de l'agent. Nous choisissons une politique Epsilon-greedy qui décroit avec le temps.\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    #attr=\"eps\" indique que nous allons donner nos spécifications pour epsilon\n",
    "    #value_max=1. indique que la valeur intiale de epsilon est 1\n",
    "    #value_min=.1 indique que la valeur finale de epsilon est 0.1\n",
    "    #value_test=.2 est plus subtil. A chaque étape, on tire un nombre entre 0 et 1 pour savoir si on choisit l'action qui maximise Q ou si on choisit d'explorer. Si ce nombre tiré \n",
    "    #est plus petit que value_test, alors on décide de diminuer epsilon vers 0.1\n",
    "    #nb_steps=10000 indique le pas de descente de epsilon. Il faut 10000 étapes où le nombre est plus petit que value_test pour que epsilon passe de value_max à value_min\n",
    "\n",
    "\n",
    "    #Nous devons créer le buffer dans lequel notre agent mémorise les informations de son entrainement\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    #limit=1000 indique que notre buffer contient au maximum 1000 épisodes/ tentatives de jeu \n",
    "    #window_length=3 indique que nous utilisons 3 fenêtres de buffer pour les 1000 épisodes\n",
    "\n",
    "    #Créons donc notre agent\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "    #model: on donne à l'agent le réseau neuronal\n",
    "    #memory : on donne à l'agent le buffer de mémoire\n",
    "    #policy : on donne à l'agent la politique à suivre\n",
    "\n",
    "    #enable_dueling_network permet d'optimiser et d'accélérer le training de l'agent\n",
    "    #dueling_type permet d'optimiser et d'accélérer le training de l'agent\n",
    "\n",
    "    #nb_actions=actions : indique à l'agent le nombre d'acions -> ici 6\n",
    "    #nb_steps_warmup: On laisse à l'agent un warmup/Echauffement de 1000 épisodes avant de commencer à effectivement entrainer l'agent dans les règles de l'art .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On initialise notre agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "\n",
    "#On utilise le processus Adam qui permet d'optimiser l'agent au départ et donc de gagner du temps. \n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "#lr correspond au learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C'est bon tout est prêt ! C'est parti, entrainons cet agent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
    "#Cette fonction permet d'entrainer l'agent sur l'environnement \"env\" pendant \"nb_steps\".\n",
    "#Attention il ne faut pas confondre un épisode qui correspond à une tentative de jeu/ un partie entière avec un \"step\" qui correspond à une unique décision prise par l'agent face à une image.\n",
    "#Il y a donc plusieurs steps par épisode. (sauf si l'agent meurt dès la première image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Voilà nous avons ainsi entrainé notre agent\n",
    "- Maintenant, il faudrait pouvoir le tester afin de comparer par la suite à d'autres politiques et voir si notre agent devient effectivement de plus en plus fort.\n",
    "Lançons donc 10 épisodes avec l'option visualize=True pour pouvoir afficher à l'écran l'agent qui joue au jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarder et réutiliser un agent depuis la mémoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Voilà, nous avons réussi à entrainer notre agent sur l'environnement SpaceInvaders. Pour qu'il devienne effectivement bon au jeu, il est capital de l'entrainer un très grand nombre de fois.\n",
    "Nous avons pas forcément le temps ni les capacités computationnelles de le faire. C'est pourquoi il est pertinent de pouvoir enregistrer notre optimisation et pouvoir la réutiliser ultérieurement. (Ne serait ce\n",
    "pour éviter de tout perdre lors d'un crash de la machine qui travaille, ou encore pour pouvoir importer un agent d'une autre personne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lalou\\Desktop\\PAF\\PAF\\KerasRL-OpenAI-Atari-SpaceInvadersv0-main\\Space Invaders Walkthrough.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lalou/Desktop/PAF/PAF/KerasRL-OpenAI-Atari-SpaceInvadersv0-main/Space%20Invaders%20Walkthrough.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39m#Voilà, nous avons réussi à entrainer notre agent sur l'environnement Space Invaders. Pour qu'il devienne effectivement bon au jeu, il faudrait l'entrainer énormément de fois. \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lalou/Desktop/PAF/PAF/KerasRL-OpenAI-Atari-SpaceInvadersv0-main/Space%20Invaders%20Walkthrough.ipynb#ch0000022?line=1'>2</a>\u001b[0m \u001b[39m#Nous avons pas forcément le temps de le faire. C'est pourquoi il est pertinent de pouvoir enregistrer notre optimisation et pouvoir la réutiliser ultérieurement. (Ne serait ce )\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lalou/Desktop/PAF/PAF/KerasRL-OpenAI-Atari-SpaceInvadersv0-main/Space%20Invaders%20Walkthrough.ipynb#ch0000022?line=2'>3</a>\u001b[0m dqn\u001b[39m.\u001b[39msave_weights(\u001b[39m'\u001b[39m\u001b[39mSavedWeights/10k-Fast/dqn_weights.h5f\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights.h5f')   \n",
    "#Notre agent est sauvegardé dans \"./SavedWeights/10k-Fast/dqn_weights.h5f\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ci dessous afin de reset notre model et notre agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Avant d'éxécuter la cellule ci dessous. Il faut re-compiler les parties 3 et 4 SAUF LA CELLULE dqn.fit qui entraîne l'agent. En effet nous allons importer un agent déjà entrainer\n",
    "à 1 million de Steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/1m/dqn_weights.h5f')\n",
    "#Compiler alors cette cellule puis exécuter la cellule qui teste l'agent pendant 10 épisodes.\n",
    "#On peut alors comparer les scores de l'agent entrainer 1 million de fois avec l'agent entrainer 10000 fois et celui du début qui prend toutes les décisions aléatoirement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "En comparant un agent entrainé à 1M de steps avec un agent qui joue aléatoirement, on observe une net différence de niveau de jeu. L'agent entrainé obtient bel est bien des meilleurs scores, ouf !\n",
    "Cependant on remarque qu'avec 1M de steps de vécu, notre agent entrainé reste pas très bon par rapport à un humain. Il faudrait donc continuer le training... cela prend du temps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bf8c1a98c83d4584ef36de191628eb3b82998d980797082e1a3558cd683f8df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
