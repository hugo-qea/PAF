{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook réalisé par Aristide LALOUX, Hugo QUENIAT et Mohamed Ali SRIR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sujet :\n",
    "\n",
    "Ce notebook contient le code nécessaire pour créer et entraîner un agent à jouer à Flappy Bird. Ici nous implémentons un algorithme de type Q-Learning.\n",
    "\n",
    "Un état est caractérisé par :\n",
    "- La distance horizontal entre l'oiseau et le prochain tuyau.\n",
    "- La distance vertical entre l'oiseau et le milieu de la prochaine ouverture. \n",
    "\n",
    "A la fin de l'entraînement, l'agent devrait jouer l'action optimale à chaque instant dans le jeu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Environnement : https://github.com/Talendar/flappy-bird-gym\n",
    "\n",
    "Code Q Learning : On s'est inspiré de celui du CartPole. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous installons le jeu.\n",
    "#Sur windows, écrire !pip plutôt que !{sys.executable}\n",
    "\n",
    "#!{sys.executable} -m pip install flappy-bird-gym\n",
    "#!{sys.executable} -m pip install torch\n",
    "\n",
    "!pip install flappy-bird-gym\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous importons les librairies utiles.\n",
    "#La bibliothèque GYM contient l'environnement de flappy bird. C'est à dire les états, les actions, les rewards\n",
    "import gym \n",
    "import flappy_bird_gym\n",
    "\n",
    "#On utilise numpy pour les matrices \n",
    "import numpy as np\n",
    "\n",
    "#Random pour les politiques epsilon-greedy.\n",
    "import random\n",
    "\n",
    "#Maths pour faire des fonctions avancées\n",
    "import math\n",
    "\n",
    "#Pour l'affichage des graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Time pour faire des sleep(n) et ainsi gérer le nombre de FPS du jeu. \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Création et inspection de l'environnement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un état : \n",
    "- La distance horizontale entre l'oiseau et le prochain tuyau.\n",
    "- La distance verticale entre l'oiseau et le milieu de la prochaine ouverture. \n",
    "\n",
    "Les actions :\n",
    "- NOOP : Ne rien faire.\n",
    "- JUMP : Faire un saut.\n",
    "\n",
    "Rewards : \n",
    "- Distance horizontale parcourue entre deux états\n",
    "- Nous modifions l'environnement gym pour mettre une récompense négative lorsque l'agent meurt. Ainsi, on espère que l'agent apprendra qu'il ne faut pas mourir dans le jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de l'environnement Gym du jeu Flappy Bird\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-inf, inf), (-inf, inf)]\n",
      "2\n",
      "(2, 2)\n",
      "-inf\n"
     ]
    }
   ],
   "source": [
    "#Ici nous enquêtons sur la nature de l'environnement Flappy Bird de Gym en affichant les intervalles d'évolution de DeltaX et DeltaY qui caractérisent nos états\n",
    "state_value_bounds = list(zip(env.observation_space.low, \n",
    "                              env.observation_space.high)) \n",
    "\n",
    "print(state_value_bounds)\n",
    "print(len(state_value_bounds))\n",
    "print(np.shape(state_value_bounds))\n",
    "print(state_value_bounds[0][0])\n",
    "#On remarque que DeltaX et DeltaY évolue dans R tout entier. \n",
    "#Nous allons donc discrétiser l'espace de jeu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code pour tester que l'environnement fonctionne correctement.\n",
    "# L'agent prend ici des actions totalement aléatoires.\n",
    "#On remarque qu'il finit au sommet de la zone de jeu, le saut a beaucoup plus \"d'impact\" que le \"ne rien faire\".\n",
    "\n",
    "#Boucle infinie : le jeu se répète tant que la cellule n'est pas interrompue.\n",
    "while True :\n",
    "    #Mise en place d'une séquence de jeu\n",
    "\tobs = env.reset()\n",
    "    #Boucle infinie qui peut seulement être brisée si l'agent meurt.\n",
    "\twhile True:\n",
    "\t\t# On sélectionne la prochaine action.\n",
    "        # La commande \"env.action_space.sample\" permet de tirer au sort une action parmi l'ensemble des actions possibles de l'environnement\n",
    "\t\taction = env.action_space.sample() \n",
    "\t\t\n",
    "\t\t# L'environnement fait éxécuter l'action par l'agent.\n",
    "\t\tobs, reward, done, info = env.step(action)\n",
    "\t\t\n",
    "\t\t# print(obs) permet d'afficher les états que rencontre l'agent à chaque étape.\n",
    "        # Affichage du jeu\n",
    "\t\tenv.render()\n",
    "\n",
    "\t\ttime.sleep(1 / 60)  # FPS\n",
    "\t\t\n",
    "\t\t# Vérification de si l'agent est toujours en vie\n",
    "\t\tif done:\n",
    "\t\t\tbreak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque (en utilisant la ligne \"print(obs)\") que les états parcourus par l'agent sont en réalité des états (x,y) tels que x est dans [0,2] et y dans ]-1,1[]. Ceci va guider la création de la fonction suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combien de décimales souhaitez-vous conserver pour les états (x,y) ?\n",
    "\n",
    "Precision_DeltaX = 2\n",
    "Precision_DeltaY = 1\n",
    "\n",
    "# Avec les valeurs ci-dessus, nous avons pu observer un compromis correct entre convergence rapide et performances de l'agent.\n",
    "#Cependant, il faut faire attention à ne pas discrétiser trop précisément l'espace de jeu car cela engendrerait trop d'états et ainsi la matrice Q convergerait\n",
    "#trop lentement vers la matrice Q*. Mais d'un autre côté, si on ne discrétise pas suffisament, il n'y aura pas assez d'état, et donc l'agent ne sera pas précis\n",
    "#au pixel près. Il faut trouver empiriquement un juste milieu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de discrétisation\n",
    "#Bucketize signifie que nous discrétisons l'espace de jeu qui est à l'origine continu. \n",
    "def bucketize_state_value(state):\n",
    "\tx,y = state\n",
    "\treturn (math.floor(((10**(Precision_DeltaX-1))*x)),math.floor((10**(Precision_DeltaY))*y))\n",
    "\n",
    "#La fonction ci-dessus renvoie les valeurs discrétisées de DeltaX et DeltaY. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de la matrice Q, ses dimensions sont en adéquation avec les précisions choisies ci-dessus.\n",
    "#Plus la précision est élevée, plus Q est de grande dimension.\n",
    "#env.action_space.n donne le nombre d'actions possibles.\n",
    "#Ainsi on a bien une valeur pour chaque couple (s,a) avec s=(DeltaX,DeltaY)\n",
    "q_value_table = np.zeros((2*(Precision_DeltaX-1),2*(Precision_DeltaY)) + (env.action_space.n,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres définis par l'utilisateur pour l'algo de Q learning\n",
    "max_episodes = 5000000 #Nombre de parties que l'agent va jouer pendant l'entrainement \n",
    "streak_to_end = 120 #Pour gagner, il faut que l'agent gagne 120 parties d'affilée. \n",
    "solved_time = 500  #Nombre de décisions que l'agent peut prendre pour une partie. Au delà, on considère que l'agent a résolu le jeu.\n",
    "discount = 0.9 #Facteur de discount dans le Q Learning\n",
    "no_streaks = 0 #Au début, l'agent n'a pas encore résolu le jeu, donc notre streak / nombre de parties gagnées d'affilée vaut 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon Greedy Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection d'une action : Exploration ou Exploitation\n",
    "# Méthode epsilon-greedy\n",
    "\n",
    "min_explore_rate = 0.1 \n",
    "min_learning_rate = 0.07\n",
    "explore_rate_decay = 0.999998 #Raison de la suite géométrique que suit l'exploration_rate\n",
    "\n",
    "def select_action(state_value, explore_rate):\n",
    "\tif random.random() < explore_rate:\n",
    "\t\t#Exploration, on explore l'environnement.\n",
    "\t\tif random.random() < 0.95: #Pour éviter que l'oiseau se retrouve haut dans le ciel comme on l'a observé dans le test aléatoire. Dans 95% des cas, on ne fait rien et dans 5% des\n",
    "\t\t#cas on saute. En moyenne l'oiseau reste à peu prêt à la même altitude. \n",
    "\t\t\taction = 0 #Action NOOP\n",
    "\t\telse:\n",
    "\t\t\taction = 1 #Action JUMP\n",
    "\telse:\n",
    "\t\taction = np.argmax(q_value_table[state_value])  # Exploitation, on prend l'action qui maximise le gain. \n",
    "\treturn action\n",
    "\n",
    "def select_explore_rate(x):\n",
    "    # Actualisation de l'exploration rate au fur et à mesure.\n",
    "    return max(min_explore_rate, explore_rate_decay*x)\n",
    "\n",
    "def select_learning_rate(x):\n",
    "    # Actualisation du learning_rate au fur et à mesure.\n",
    "    return max(min_learning_rate, min(1.0, 1.0 - math.log10((x+1)/25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code à remplir, modification de la table Q selon l'algorithme de Q-Learning\n",
    "\n",
    "def update_q_table(.....): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correction du code ci-dessus\n",
    "def update_q_table(previous_state, current_state, action, learning_rate, reward):\n",
    "        best_q_value = np.max(q_value_table[current_state])\n",
    "        q_value_table[previous_state][action] += learning_rate * (\n",
    "            reward + discount * best_q_value - \n",
    "            q_value_table[previous_state][action])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On commence avec un grand Exploration Rate puis il va diminuer avec le temps, selon une décroissance géométrique de raison \"explore_rate_decay\"\n",
    "\n",
    "explore_rate = 1\n",
    "\n",
    "\n",
    "# Lancement de l'entrainement\n",
    "\n",
    "\n",
    "for episode_no in range(max_episodes):\n",
    "\n",
    "    #Pour chaque partie, on calcule les nouveaux exploration et learning rates.  \n",
    "    explore_rate = select_explore_rate(explore_rate)\n",
    "    learning_rate = select_learning_rate(episode_no)\n",
    "\n",
    "\n",
    "\n",
    "    # On reset l' environment avant le début de la partie.\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #On discrétise l'état (X,Y) avec notre fonction au dessus. \n",
    "    start_state_value = bucketize_state_value(observation)\n",
    "    previous_state_value = start_state_value\n",
    "\n",
    "\n",
    "\n",
    "    done = False \n",
    "    time_step = 0  #nombre de décisions effectuées par l'agent\n",
    "\n",
    "    #Tant que la partie n'est pas terminée, l'agent joue et essaye de faire le meilleur score possible à l'aide de l'algo du Q Learning. \n",
    "    while not done: \n",
    "        #Pour afficher l'agent en train de jouer.\n",
    "        #env.render()  \n",
    "\n",
    "        #On choisit l'action selon notre politique epsilon greedy. \n",
    "        action = select_action(previous_state_value, explore_rate)\n",
    "\n",
    "        #L'agent joue l'action et il reçoit le nouvel état la récompense associée, s'il est mort ou non et son score.\n",
    "        observation, reward_gain, done, info = env.step(action)\n",
    "\n",
    "        #On discrétise le nouvel état\n",
    "        state_value = bucketize_state_value(observation)\n",
    "\n",
    "        \n",
    "        reward_gain= 100*max(0,previous_state_value[0]-state_value[0]) - 10000 * done\n",
    "        #reward_gain= - 10000 * done\n",
    "        #Ainsi le Gain est très mauvais si l'agent meurt car done passe à 1\n",
    "        #Le gain vaut 0 si l'agent lorsque le pipe de référence change (on passe au suivant)\n",
    "\n",
    "        #Appliquer l'algorithme de Q-Learning en mettant à jour la table Q\n",
    "        update_q_table(...)\n",
    "\n",
    "        previous_state_value = state_value\n",
    "  \n",
    "        time_step += 1\n",
    "        \n",
    "    \n",
    "    if time_step >= solved_time: #Si la partie dure plus de solved_time actions, alors on considère que la partie est gagnée\n",
    "        #Etant donné que le premier tuyau et le n-ième tuyau est identique, on peut s'arrêter à 199 actions\n",
    "        no_streaks += 1\n",
    "    else:\n",
    "        no_streaks = 0\n",
    "    \n",
    "    if no_streaks > streak_to_end: #puis si l'agent gagne plus de streak_to_win parties d'affilé, alors l'entrainement est terminé. \n",
    "        print('Après {} épisodes, le fappy bird est résolu.'.format(episode_no))\n",
    "        break\n",
    "\n",
    "    # Données à afficher en console, avec un certain pas (de 100 épisodes ici)\n",
    "    if episode_no % 100 == 0:  \n",
    "        print('Episode {} terminé après {} décisions, un exploration_rate de {}'.format(episode_no, time_step, explore_rate))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Q Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons sauvegarder dans le fichier \"Qtensor.pt\" la matrice Q pour l'utiliser ultérieurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "print(q_value_table)\n",
    "torch.save(q_value_table, 'Qtensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Q Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons load une matrice Q Tensor déjà entrainée pour la tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "q_value_table = torch.load(\"Qtensor.pt\")\n",
    "print(q_value_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "41a6b8a033e89ba70e537d21b1c6eca9b2672d4473698e58322a2249e5677df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
