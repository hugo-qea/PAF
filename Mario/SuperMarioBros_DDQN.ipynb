{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notebook réalisé par Aristide LALOUX, Hugo QUENIAT et Mohamed Ali SRIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliographie\n",
    "\n",
    "- Deep Reinforcement Learning with Double Q-learning, Hado V. Hasselt et al, NIPS 2015: https://arxiv.org/abs/1509.06461\n",
    "- Train a Mario-playing RL Agent, Yuansong Feng, Suraj Subramanian, Howard Wang et Steven Guo,  GitHub 2020 : https://github.com/pytorch/tutorials/blob/master/intermediate_source/mario_rl_tutorial.py\n",
    "- Super Mario Bros for OpenAI Gym, Christian Kauten, GitHub 2018 : https://github.com/Kautenja/gym-super-mario-bros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise en place de l'environnement de jeu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement de l'environnement du jeu, de l'émulateur\n",
    "\n",
    "\tNotre simulation va avoir lieu sur SuperMarioBros donc nous chargeons le jeu adapté. En outre, OpenAI Gym dispose de nombreux environnements de jeux anciens (Atari, NES etc) que vous pouvez utiliser avec une IA entrainée de façon analogue.\n",
    "\n",
    "\tInitialement, le jeu permet, en comptabilisant toutes les combinaisons d'actions possibles sur un controller NES, d'effectuer 256 actions distinctes. Cependant, dans un tel cas la table Q atteint des dimmensions bien trop élevées. Dès lors, on utilise la fonction JoypadSpace qui permet de simplifier les commandes possibles au sein du jeu.  L'ensemble des actions permises est ainsi réduit à 7 actions, les suivantes :\n",
    "\n",
    "['NOOP'] (aucun mouvement)\n",
    "\n",
    "['right'] (l'agent se déplace sur la droite)\n",
    "\n",
    "['right', 'A'] (l'agent se déplace sur la droite et saute)\n",
    "\n",
    "['right', 'B']  (l'agent se déplace sur la droite et envoie un projectile)\n",
    "\n",
    "['right', 'A', 'B'] (l'agent se déplace sur la droite, saute et envoie un projectile)\n",
    "\n",
    "['A'] (l'agent saute)\n",
    "\n",
    "['left'] (l'agent se déplace sur la gauche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances et de Mario\n",
    "\n",
    "!{sys.executable} -m pip install gym\n",
    "!{sys.executable} -m pip install gym_super_mario_bros==7.3.0 nes_py\n",
    "!{sys.executable} -m pip install torch torchvision torchaudio\n",
    "!{sys.executable} -m pip intall pathlib\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des dépendances et du jeu\n",
    "\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import random\n",
    "import math\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération du jeu dans sa configuration avec 7 actions accessibles\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test du bon fonctionnement du jeu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisateur a deux options pour exécuter ce test:\n",
    " - laisser l'agent choisir de façon complètement aléatoire chaque action (pour chaque état la politique de décision est uniforme)\n",
    " - déterminer sa propre politique de décision qui sera appliquée pour chaque état"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres modifiables par l'utilisateur pour le test\n",
    "No_Images = 10000 #Nombre d'images à parcourir avant que la simulation ne prenne fin.\n",
    "Random_Actions = True #Indiquer si on souhaite voir l'IA prendre des actions complètement aléatoires\n",
    "Actions_Law= [0, 1, 0, 0, 0, 0, 0] #loi de probabilité sur l'action à prendre à tout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancement du jeu en fonction des paramètres choisis ci-dessus\n",
    "\n",
    "if not Random_Actions:\n",
    "\tlaw=[]\n",
    "\tfor i in range(7):\n",
    "\t\tlaw+=[i]*math.floor(100*Actions_Law[i])\n",
    "done = True\n",
    "# Jouer un nombre 'No_Images' d'images du jeu (frames)\n",
    "for step in range(No_Images):\n",
    "\tif done :\n",
    "\t\tenv.reset()\n",
    "\tif Random_Actions :\n",
    "\t\taction = env.action_space.sample()\n",
    "\telse : \n",
    "\t\taction = law[random.randint(0,99)]\n",
    "\tstate, reward, done, info = env.step(action)\n",
    "\tenv.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation de l'agent via Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des dépendances pour prétraiter les images du jeu\n",
    "\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation\n",
    "from gym.spaces import Box\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from collections import deque\n",
    "import datetime, os, copy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couches de prétraitement\n",
    "- Prise en compte d'une seule image par paquet reçu.\n",
    "- Traitement des images prises en compte en groupe.\n",
    "- Décolorisation des images, passage en niveau de gris (Réduction par 3 de la dimension, RGB -> Gris)\n",
    "- Sous-échantillonnage des images pour conserver une image de taille inférieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de prétraitement modifiables par l'utilisateur\n",
    "# Néanmoins, nous vous conseillons de ne pas trop toucher à ces paramètres (surtout à la taille des images)\n",
    "pas_Images = 4 # Une action concerne pas_Images (autrement dit, un nombre pas_Images est considéré comme une seule image)\n",
    "paquet_Images = 4 # Le traitement dans le réseau de neurones verra en entrée un paquet de paquet_Images images. Cela permet d'inclure la notion de mouvement du personnage\n",
    "taille = 84 # Paramètre de sous-échantillonnage de l'image, ne conserver que taille x taille pixels pour l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couches de prétraitement\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        # Afin d'accélérer le processus de convergence nous n'utilisons que 1 image/skip.\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        # Comme nous n'utilisons qu'une image toutes les skip images, durant ces skip images nous utilisons la même action\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulation des récompenses et répétition de la même actiion durant les skip images\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return state, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # Conversion du tableau [Hauteur, Largeur, Couleur] en un tenseur [Couleur, Hauteur, Largeur]\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "\t\t# Conversion de l'image en niveaux de gris.\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\t\t#Dimension de l'espace voulu\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "\t\t#Normalisation des valeurs depuis [0,255] à [0,1] puis sosu échantillonnage à la taille demandée\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Application des couches de prétraitement à notre environnement\n",
    "env = SkipFrame(env, skip=pas_Images)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=taille)\n",
    "env = FrameStack(env, num_stack=paquet_Images) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuration de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des méthodes de l'agent\n",
    "\n",
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        #Choix de l'action en fonction de l'état state de l'agent et la tactique epsilon-greedy\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        #Permet d'ajouter à sa mémoire l'expérience\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        #Faire appel aux expériences précédentes via la mémoire de l'agent\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        #Copie à partir des expériences dans la table Qtarget\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "À chaque état, en fonction du ${\\epsilon}$ de la stratégie ${\\epsilon}$-greedy, l'agent peut choisir d'effectuer l'action la plus optimale selon sa table $Q$ (on dit qu'il exploite) ou une action totalement aléatoire (on dit qu'il explore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres modifiables par l'utilisateur pour l'entrainement de l'agent\n",
    "exploration_rate_initial = 1 #Le epsilon de la stratégie epsilon-greedy. Il est en général mis à une valeur très élevée (très proche de 1 ou égale à 1) au début de l'entrainement de l'entrainement afin que l'agent puisse explorer toutes les actions et les états.\n",
    "exploration_rate_coefficient_multiplicateur = 0.99999975 #Raison de la suite géométrique définissant la décroissance de l'exploration rate\n",
    "exploration_rate_min = 0.1 #Exploration_rate minimal, au minimum l'agent fera une action aléatoire avec probabilité exploration_rate_min\n",
    "Evaluation_Mode = False #Souhaite-t-on évaluer l'agent ?\n",
    "Periodic_Evaluation = True #Souhaite-t-on périodiquement évaluer l'agent ?\n",
    "Evaluation_every_step = 10 #Pas d'évaluation\n",
    "pas_enregistrement = 5 #A quelle fréquence le log doit-il enregistrer les valeurs observées durant un épisode\n",
    "sauvegarde_Mémoire = 1e5 # Nombre d'actions nécessaires de l'agent avant qu'il exécute une sauvegarde du réseau de neurones en mémoire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion du choix et du traitement de l'action à effectuer\n",
    "\n",
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Le réseau de neurones profond qui permet à Mario de prédire l'action optimale à réaliser\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = exploration_rate_initial\n",
    "        self.exploration_rate_decay = exploration_rate_coefficient_multiplicateur\n",
    "        self.exploration_rate_min = exploration_rate_min\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = sauvegarde_Mémoire\n",
    "\n",
    "    def act(self, state):\n",
    "\t\t#Choix de l'action suivant la stratégie epsilon-greedy\n",
    "        # EXPLORATION\n",
    "        if np.random.rand() < self.exploration_rate and (not Evaluation_Mode):\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOITATION\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # Cécroissaince de l'exploration_rate jusqu'au minimum choisi.\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # Incrémentation du nombre d'étapes\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement de l'agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fondement de l'algorithme de Double DQN\n",
    "\n",
    "L'algorithme de Double DQN s'appuie sur le Double Q-learning et le DQN afin de former une méthode d'apprentissage profond par renforcement qui limite la surestimation de l'action value fonction.\n",
    "\n",
    "Premièrement, l'héritage des Deep Q Networks (DQN) vient de l'utilisation d'un réseau dit \"target\" (cible), d'un réseau dit \"online\" (en ligne) et de l'experience replay. Ainsi, l'experience replay est utilisé afin de mettre à jour le réseau online. Le réseau target est ensuite périodiquement copié sur le réseau online. Néanmoins, chaque approximation dans l'équation de Bellman se fait en utilisant le réseau target, les mises à jour courantes ne sont ainsi rééutilisées que plus tard.\n",
    "\n",
    "Deuxièmement, l'héritage du Double Q-learning provient de l'apprentissage de deux tables Q distinctes, associées à deux séries de poids : les poids du réseau online et les poids du réseau target. Les poids du réseau online servent à choisir la greedy policy tandis que les poids du réseau target permettent de l'évaluer.\n",
    "\n",
    "Finalement, on utilise la table/ le réseau (ils sont associés) online pour évaluer la greedy policy à utiliser tandis qu'on utilise la table/ le réseau target pour en déterminer la valeur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay\n",
    "\n",
    "Afin de mettre à jour les tables $Q_{target}$ et $Q_{online}$, l'agent fera appel (voir cellules suivantes) à l'experience replay : il apprend d'un certain échantillon de ses expériences précédentes pour déterminer les meilleures stratégies à adopter. Nous mettons ainsi en place un processus de sauvegarde en mémoire et d'accès à cette même mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gestion de la mémoire\n",
    "\n",
    "class Mario(Mario):  #sous-classe pour hériter des méthodes et attributs pré-annoncées et déjà écrits\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000) #la mémoire de l'agent\n",
    "        self.batch_size = 32 #nombre d'expériences à utiliser pour mettre à jour la table Qtarget à chaque accès à la mémoire\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "\t\t#L'entrée est en LazyFrame pour state et next_state, format utilisé par Gym. Il est nécessaire de les convertir en array Numpy pout utiliser torch.\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "\t\t#Pytorch a deux options, utiliser le GPU (Cuda) ou le CPU (pas Cuda)\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\t\t#Stockage en mémoire (un buffer en fait) des expériences de l'agent\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "\t\t#On recueille un échantillon aléatoire des expériences stockées en mémoire pour mettre à jour la table Qtarget\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - Réseau de neurones convolutionnel\n",
    "\n",
    "Le réseau utilisé ici consiste en trois convolutions suivies d'un Relu, puis d'un flatten et enfin de deux couches fully connected linéaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description du réseau de neurones convolutionnel\n",
    "\n",
    "class MarioNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\t\t#s'assurer que les dimensions de l'image en entrée du CNN sont bien conformes à ce qui est attendu post-prétraitement.\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\t\t#Le CNN en lui-même\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Les paramètres de la table Q_target restent inchangés, on indique donc de ne pas l'impliquer dans la descente de gradient.\n",
    "\t\t# Elle sera synchronisée avec Q_online périodiquement.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du réseau de neurones pour utilisation ultérieure\n",
    "\n",
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application de la méthode du Double DQN\n",
    "\n",
    "- TD estimate : estimation de la greedy policy pour un état s\n",
    "#\n",
    "- - ${TD}_e = Q_{online}^*(s,a)$\n",
    "#\n",
    "#\n",
    "- TD Target : détermination de la valeur de la greedy policy à partir de l'état s' qui suit et r le reward courant\n",
    "#\n",
    "- - $a' = argmax_{a} Q_{online}(s', a)$\n",
    "- - ${TD}_t = r + \\gamma Q_{target}^*(s',a')$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de TD estimate et TD target\n",
    "\n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "\t\t# Discount factor, permet d'assurer la convergence de la fonction de gain J(pi)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # TD_estimate = Q_online(s,a); on choisit la valeur maximale de la table Q online pour l'état s dans lequel on se trouve.\n",
    "        return current_Q\n",
    "\t# Ne pas faire de descente de gradient sur le réseau target\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "\t\t# On détermine l'état suivant\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "\t\t# On détermine l'action à prendre dans l'état suivant\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "\t\t# On détermine la valeur de Q_target associée\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float() # TD_target = current_reward + gamma *  Q_target(s',a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise à jour du modèle\n",
    "\n",
    "On applique l'algorithme de descente de gradient pour mettre à jour les poids du réseau online (qui seront ensuite périodiquement copiés sur le réseau target). On note $\\alpha$ le learning rate, dans ]0;1] il permet de limiter ou non l'impact du bruit.\n",
    "\n",
    "- - $\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise à jour du modèle\n",
    "\n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "\t\t#Algorithme de descente de gradient avec un learning rate lr : algortihme d'Adam\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "\t\t#Mesure de différence dans une série statistique, écart L^1 quand l'écart est supérieur à 1 et écart quadratique sinon\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "\t\t#On mesure la différence entre les tables online et target \n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "\t\t#On vide le gradient avant de le calculer à nouveau\n",
    "        self.optimizer.zero_grad()\n",
    "\t\t#Calcul du gradient\n",
    "        loss.backward()\n",
    "\t\t#Les paramètres du réseau de neurones sont mises à jour à partir de ce gradient\n",
    "        self.optimizer.step()\n",
    "\t\t#Renvoi de la différence, qui sera ensuite exploitée pour mesurer l'efficacité de l'épisode\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "\t\t#Synchronisation de la table Q_target\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase d'apprentissage et de reports des expériences sur les modèles\n",
    "\n",
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # nombre d'expériences avant le début de l'entrainement\n",
    "        self.learn_every = 3  # chaque mise à jour de Q_online se fait tous les learn_every\n",
    "        self.sync_every = 1e4  # période de synchronisation de Q_target\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "        #Extraction d'une expérience depuis la mémoire\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        td_est = self.td_estimate(state, action)\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\t\t#Mise à jour de la table Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt) # on utilise\n",
    "\t\t#Renvoi de la moyenne de la table Q sur cet épisode et de la différence avec la table target\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lancement du jeu pour évaluer l'IA ou l'entrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde de données de l'IA pour étude, posttraitement humain et réutilisation ultérieure.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "\t\t#Rapport d'un épisode à transcrire sur le fichier log\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "\t\t#Diffèrents graphiques\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # Évolution des données qui permettent d'évaluer l'agent et son entrainemen t\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moyennes glissantes\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Début de l'enregistrement d'un épisode\n",
    "        self.init_episode()\n",
    "\n",
    "        # Heure de l'enregistrement\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\t#Ajout à l'enregistrement des valeurs observées par l'action effectuée\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\t#Après la fin de l'épisode, ajoutde toutes les valeurs ajoutées aux différentes historiques)\n",
    "    def log_episode(self):\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\t\t#fin des enregistrements, lancement d'un nouvel enregistrement pour l'épisode suivant\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "\t\t#Au début de l'épisode tous les compteurs sont réinitialisés\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "\t\t#Calcul des moyennes glissantes et ajouts\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\t\t\n",
    "\t\t#Détermination des temps de calcul entre deux instants d'enregistrement\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "\t\t#Affichage dans la console et sur le log\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\t\t#Mise à jour des graphiques\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire jouer l'IA au jeu\n",
    "\n",
    "#Utilisation du GPU NVIDIA si possible pour accélérer le processus\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "#Création du dossier pour sauvegarder les graphiques, log et checkpoints\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "#Notre agent\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "#Classe permettant la sauvegarde des infos\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "#Mode : training ou evaluation\n",
    "evaluation = Evaluation_Mode\n",
    "\n",
    "#Nombre d'épisodes à entrainer l'agent\n",
    "episodes = 40\n",
    "for e in range(episodes):\n",
    "\t#Réinitialisation du monde pour nouvel essai\n",
    "\tstate = env.reset()\n",
    "\t#Début du jeu\n",
    "\t#On décide d'afficher une tentative périodiquement pour voir l'évolution ou à chaque épisode si on ets dans un mode d'évaluation\n",
    "\tevaluation = (e%Evaluation_every_step== 0 and Periodic_Evaluation) or (Evaluation_Mode) \n",
    "\twhile True:\n",
    "\t\tif (mario.curr_step % mario.save_every == 0) :\n",
    "\t\t\tmario.save()\n",
    "\t\tif (not evaluation):\n",
    "\t\t\t# L'agent choisit l'action à effectuer en fonction de l'état dans lequel il est\n",
    "\t\t\taction = mario.act(state)\n",
    "\t\t\t# L'agent applique l'action dans le jeu\n",
    "\t\t\tnext_state, reward, done, info = env.step(action)\n",
    "\t\t\t# L'agent enregistre l'expérience effectuée\n",
    "\t\t\tmario.cache(state, next_state, action, reward, done)\n",
    "\t\t\t# L'agent apprend \n",
    "\t\t\tq, loss = mario.learn()\n",
    "\t\t\t# Enregistrement de l'évolution\n",
    "\t\t\tlogger.log_step(reward, loss, q)\n",
    "\t\t\t# Passage au nouvel état\n",
    "\t\t\tstate = next_state\n",
    "\t\telse:\n",
    "\t\t\t# L'agent choisit l'action à effectuer en fonction de l'état dans lequel il est\n",
    "\t\t\taction = mario.act(state)\n",
    "\t\t\t# L'agent applique l'action dans le jeu\n",
    "\t\t\tnext_state, reward, done, info = env.step(action)\n",
    "\t\t\t#On affiche l'évaluation\n",
    "\t\t\tenv.render()\n",
    "\t\t\t# L'agent enregistre l'expérience effectuée\n",
    "\t\t\tmario.cache(state, next_state, action, reward, done)\n",
    "\t\t\t# Enregistrement de l'évolution\n",
    "\t\t\tlogger.log_step(reward, loss, q)\n",
    "\t\t\t# Passage au nouvel état\n",
    "\t\t\tstate = next_state\n",
    "\t\t# Vérifier si le jeu est fini (réussite ou mort)\n",
    "\t\tif done or info[\"flag_get\"]:\n",
    "\t\t\tif evaluation:\n",
    "\t\t\t\tenv.close()\n",
    "\t\t\tbreak\n",
    "\t# Enregistrement de l'épisode\n",
    "\tlogger.log_episode()\n",
    "\t# Affichage de l'enregistrement tous les pas_enregistrement\n",
    "\tif e % pas_enregistrement == 0:\n",
    "\t\tlogger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
